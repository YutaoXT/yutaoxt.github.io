<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>用最简单的方式本地部署Deepseek</title>
      <link href="/posts/32786.html"/>
      <url>/posts/32786.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="用最简单的方式本地部署Deepseek"><a href="#用最简单的方式本地部署Deepseek" class="headerlink" title="用最简单的方式本地部署Deepseek"></a>用最简单的方式本地部署Deepseek</h1><blockquote><p>前言：最近Deepseek的热度比较高，就想着写这篇文章了，网上有很多类似的教程，这里作为一个经验总结。本文章将介绍本地部署以及在其他设备上远程使用，该教程适用于Windows平台</p></blockquote><hr><h2 id="安装Ollama"><a href="#安装Ollama" class="headerlink" title="* 安装Ollama"></a>* 安装Ollama</h2><p>前往官网下载 <a href="https://ollama.com/">Ollama</a></p><p><img src="/posts/32786/ollamawg.png" alt="Ollama官网"></p><p>下载完成后直接点开exe安装（应该不用细讲这个了吧）</p><blockquote><p>如果直接点击安装，将默认安装至C盘下</p></blockquote><p>安装完成之后，同时按下Win+R打开运行窗口，输入”cmd”并回车弹出命令提示符窗口。</p><p>输入相关指令会出现类似效果，如：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama list</span><br></pre></td></tr></table></figure><p><img src="/posts/32786/cmdlist.png" alt="指令效果"></p><blockquote><p>这是已经下载Deepseek-R1的模型了，刚安装完ollama是没有东西的</p></blockquote><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>Ollama还有以下其他指令:</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ollama serve       #启动ollama</span><br><span class="line">ollama create      #从模型文件创建模型</span><br><span class="line">ollama show        #显示模型信息</span><br><span class="line">ollama run         #运行模型</span><br><span class="line">ollama pull        #从注册表中拉取模型</span><br><span class="line">ollama push        #将模型推送到注册表</span><br><span class="line">ollama list        #列出模型</span><br><span class="line">ollama cp          #复制模型</span><br><span class="line">ollama rm          #删除模型</span><br><span class="line">ollama <span class="built_in">help</span>        #获取有关任何命令的帮助信息</span><br></pre></td></tr></table></figure><hr><h2 id="下载Deepseek-R1模型"><a href="#下载Deepseek-R1模型" class="headerlink" title="*下载Deepseek-R1模型"></a>*下载Deepseek-R1模型</h2><p>我们回到下载ollama的网站，在Download上方有下载模型的地址,将标注横线的模型名称点击即可转到对应有拉取模型指令的页面<br><img src="/posts/32786/ollamawgmodels.png"></p><p><img src="/posts/32786/deepseek.png" alt="拉取DeepSeek-R1页面"></p><blockquote><p>红箭头所指按钮可更换模型参数，蓝箭头所指按钮可复制命令</p></blockquote><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">## 不要全部复制，不要全部复制，不要全部复制，</span><br><span class="line">## &gt;所指一条指令复制即可</span><br><span class="line">## <span class="number">1</span>.<span class="number">5</span>b模型</span><br><span class="line">&gt; ollama run deepseek-r1:<span class="number">1</span>.<span class="number">5</span>b</span><br><span class="line"></span><br><span class="line">## <span class="number">7</span>b模型</span><br><span class="line">&gt; ollama run deepseek-r1:<span class="number">7</span>b</span><br><span class="line"></span><br><span class="line">## <span class="number">8</span>b模型</span><br><span class="line">&gt; ollama run deepseek-r1:<span class="number">8</span>b</span><br><span class="line"></span><br><span class="line">## <span class="number">14</span>b模型</span><br><span class="line">&gt; ollama run deepseek-r1:<span class="number">14</span>b</span><br><span class="line"></span><br><span class="line">## <span class="number">32</span>b模型</span><br><span class="line">&gt; ollama run deepseek-r1:<span class="number">32</span>b</span><br><span class="line"></span><br><span class="line">## <span class="number">70</span>b模型</span><br><span class="line">&gt; ollama run deepseek-r1:<span class="number">70</span>b</span><br><span class="line"></span><br><span class="line">## <span class="number">671</span>b模型</span><br><span class="line">&gt; ollama run deepseek-r1:<span class="number">671</span>b</span><br><span class="line"></span><br><span class="line">## 请视个人情况选择模型</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>详情参照以下表格</p></blockquote><table><thead><tr><th>DeepSeek模型版本</th><th>参数量</th><th>硬件配置</th></tr></thead><tbody><tr><td>DeepSeek-R1-1.5B</td><td>1.5B</td><td>4核处理器、8G内存，无需显卡</td></tr><tr><td>DeepSeek-R1-7B</td><td>7B</td><td>8核处理器、16G内存，Ryzen7或更高，RTX 3060（12GB）或更高</td></tr><tr><td>DeepSeek-R1-8B</td><td>8B</td><td>8核处理器、16G内存，Ryzen7或更高，RTX 3060（12GB）或4060</td></tr><tr><td>DeepSeek-R1-14B</td><td>14B</td><td>i9-13900K或更高、32G内存，RTX 4090（24GB）或A5000</td></tr><tr><td>DeepSeek-R1-32B</td><td>32B</td><td>Xeon 8核、128GB内存或更高，2-4张A100（80GB）或更高</td></tr><tr><td>DeepSeek-R1-70B</td><td>70B</td><td>Xeon 8核、128GB内存或更高，8张A100&#x2F;H100（80GB）或更高</td></tr><tr><td>DeepSeek-R1-671B</td><td>671B</td><td>64核、512GB或更高，8张A100&#x2F;H100</td></tr><tr><td></td><td></td><td></td></tr></tbody></table><blockquote><p>表格只显示个别内容，内容出处<a href="https://www.mabiji.com/deepseek/deepseekr115b7b8b14b32b70b671b.html">deepseek-r1的1.5b、7b、8b、14b、32b、70b和671b有啥区别？</a></p></blockquote><p>回到cmd窗口，右键一下即可粘贴指令，并回车，等待下载模型，下载之后即可直接对话</p><p><img src="/posts/32786/cmdmodels.png" alt="deepseek r1 1.5b部署"></p><blockquote><p>该图以DeepSeek-R1:1.5b作为演示，下载并自动加载后输入“你好”的样子</p></blockquote><hr><h2 id="更加便捷的本地使用模型"><a href="#更加便捷的本地使用模型" class="headerlink" title="*更加便捷的本地使用模型"></a>*更加便捷的本地使用模型</h2><p>先下载Cherry Studio并安装<br>地址：<a href="https://kkgithub.com/CherryHQ/cherry-studio/releases/download/v0.9.30/Cherry-Studio-0.9.30-setup.exe">Cherry Studio</a></p><p>安装后并打开程序<br><img src="/posts/32786/CSused1.png" alt="CS的使用1"></p><p>如图所示，先点击左下角的齿轮小图标（红箭头），再点击Ollama选项（橙箭头），再点击启用按钮（蓝箭头），再点击管理按钮（紫箭头）。</p><p><img src="/posts/32786/CSused2.png" alt="CS的使用2"></p><blockquote><p>原先下载了14b模型，所以出现了14b和1.5b</p></blockquote><p>选择下载的模型，即按下模型名称后面的加号,可以都选择，即按下所有的加号（图二红箭头）</p><p><img src="/posts/32786/CSused3.png" alt="CS的使用3"></p><blockquote><p>按成”-“号即可</p></blockquote><p><img src="/posts/32786/CSused4.png" alt="CS的使用4"></p><p>现在就可以按下左上角聊天按钮（红箭头），并按下蓝箭头所指的按钮选择下载的模型了（蓝箭头）</p><p><img src="/posts/32786/CSused5.png" alt="CS的使用5"></p><blockquote><p>选择自己下载的模型，即可聊天</p></blockquote><h2 id="使用局域网内设备或者在局域网外环境下使用"><a href="#使用局域网内设备或者在局域网外环境下使用" class="headerlink" title="*使用局域网内设备或者在局域网外环境下使用"></a>*使用局域网内设备或者在局域网外环境下使用</h2><h3 id="变量修改"><a href="#变量修改" class="headerlink" title="*变量修改"></a>*变量修改</h3><p><img src="/posts/32786/netused1.png" alt="网络上使用1"></p><p>打开设置，并依次点击 系统&gt;高级系统设置&gt;环境变量</p><p><img src="/posts/32786/netused2.png" alt="网络上使用2"></p><p>并且依次新建两个用户环境变量，即点击新建输入变量名以及变量值并按下确定</p><blockquote><p>第一个变量</p></blockquote><figure class="highlight plaintext"><figcaption><span>Name</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OLLAMA_HOST</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>Value</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.0.0.0</span><br></pre></td></tr></table></figure><blockquote><p>第二个变量</p></blockquote><figure class="highlight plaintext"><figcaption><span>Name</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OLLAMA_ORIGINS</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>Value</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*</span><br></pre></td></tr></table></figure><p>操作后将会和上图的环境变量那样，便可以关闭这些窗口了</p><blockquote><p>注意：请阅读完后操作<br>请重新启动Ollama程序，在任务栏右下角的ollama图标右键并按下“Quit Ollama”，之后在开始或桌面上再次打开Ollama程序即可。此时如果弹出防火墙警告，请勾选专用网络和公用网络两者并点击允许访问</p></blockquote><h3 id="防火墙"><a href="#防火墙" class="headerlink" title="防火墙"></a>防火墙</h3><p>在上一步已经进行了相关的操作，表明在上一步里已经设置完了，或者也可以直接选择直接关闭防火墙（不建议，但后续API无法调用的时候这样是最直接的解决方法）或者在防火墙设置里开放11434TCP端口（按理弄完两种方式之后就可以了，之后再另作补充吧）</p><h3 id="其他设备的处理"><a href="#其他设备的处理" class="headerlink" title="其他设备的处理"></a>其他设备的处理</h3><p>在另外的设备上的操作<br>点击下载设备对应系统版本并安装和运行<a href="https://chatboxai.app/zh#download">ChatBox</a></p><p>以电脑为例（手机也是类似操作）<br>点击”Just chat”<br><img src="/posts/32786/chatbox1.png" alt="chatbox1"><br>在特定模型设置中，<br>模型提供方选择“OLLAMA API”<br>*API域名的输入：<br> · 如果是局域网内</p><p> 在运行模型的电脑上按下Win+R，弹出运行窗口，输入cmd并回车，在cmd窗口里输入该指令并回车<br> <figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ipconfig</span></span><br></pre></td></tr></table></figure><br> 弹出如下界面<br> <img src="/posts/32786/cmdnet1.png" alt="cmdnet1"><br> 框住的部分为局域网内的的IP地址，在对话的设备上的chatbox设置里的API域名输入<br> <figure class="highlight plaintext"><figcaption><span>setting</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;IP地址&gt;:11434</span><br></pre></td></tr></table></figure></p><blockquote><p>记得将&lt;IP地址&gt;替换为上述操作所获取的IP地址，IPV4或IPV6皆可。</p></blockquote><p> · 如果局域网外使用<br> 如果运行模型的设备有公网IP，可直接使用该IP，即上述操作里的&lt;IP地址&gt;替换为该公网IP,并设置运行模型地设备所处网络中的光猫的防火墙设置，详细请自行查阅，毕竟光猫大家可能是不一样的<br> 如果无公网IP，可使用Frp内网映射软件进行端口映射，在chatbox设置里将API域名整个改为端口映射地址</p><blockquote><p>免费的Frp软件如：OpenFRP等等</p></blockquote><p>处理完API域名后，不出意外的话，在chatbox设置的模型一栏即可出现模型名称，即可选择并保存</p><h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>这样你就可以使用这个DeepSeek模型了，本地模型可能有一定的局限，可以通过一些其他方法进行完善。本文章暂不演示。<br>该教程也适用于部署其他Ollama支持的模型。</p><p>感谢你的阅读</p>]]></content>
      
      
      
        <tags>
            
            <tag> ollama </tag>
            
            <tag> DeepSeek </tag>
            
            <tag> DeepSeek-R1 </tag>
            
            <tag> AI本地部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/16107.html"/>
      <url>/posts/16107.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
